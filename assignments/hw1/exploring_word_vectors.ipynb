{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/Sahil/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# All the dependencies you need, can be installed by running .\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word Vectors (30 Points)\n",
    "Word Vectors are often utilized as the fundamental component for downstream NLP tasks, e.g. question answering, text generation, etc. so it is important to build some intuitions as to their strengths and weaknesses. Here, you shall first explore word vectors, derived from co-occurrence matrices, and then those derived via word2vec. \n",
    "\n",
    "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to fact that we are encoding aspects of the word's meaning in a lower dimensional space. As Wikipedia states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\" (1). Additionally throughout the Jupyter notebook we will connote vectors with \"[ ... ]\" notation. We provide references for the referenced material throughout the notebook, as denoted by the (...) notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Count-Based Word Vectors\n",
    "Most word vector derivations and implementations are underscored by the following idea:\n",
    "\n",
    "*You shall know a word by the company it keeps (Firth, J. R. 1957:11)* (2)\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words, i.e. synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e. contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices (3,4):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-Occurrence\n",
    "Finally, we can take word-entity comparisons down to the word-word level, i.e. a co-occurrence matrix. With a fixed window of size $n$, examine $w_i$ in a document. Now examine the $n$ preceding and subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$ which fall inside the fixed window. Maintain a symmetric word-word matrix, M, where you update the count for entry M[i,j] if $w_j$ appears inside $w_i$'s window \n",
    "\n",
    "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
    "\n",
    "Document 1: \"All that glitters isn't gold\"\n",
    "\n",
    "Document 2: \"All's well that ends well\"\n",
    "\n",
    "\n",
    "|    *    | START | all | that | glitter | isn't | gold | all's | end | well | END |\n",
    "|---------|-------|-----|------|---------|-------|------|-------|-----|------|-----|\n",
    "| START   | 0     | 1   | 0    | 0       | 0     | 0    | 1     | 0   | 0    | 0   |\n",
    "| all     | 1     |   0 | 1    | 0       | 0     | 0    | 0     | 0   | 0    | 0   |\n",
    "| that    | 0     | 1   | 0    | 1       | 0     | 0    | 0     | 1   | 0    | 0   |\n",
    "| glitter | 0     | 0   | 1    | 0       | 1     | 0    | 0     | 0   | 1    | 0   |\n",
    "| isn't   | 0     | 0   | 0    | 1       | 0     | 1    | 0     | 0   | 0    | 0   |\n",
    "| gold    | 0     | 0   | 0    | 0       | 1     | 0    | 0     | 0   | 0    | 1   |\n",
    "| all's   | 1     | 0   | 0    | 0       | 0     | 0    | 0     | 0   | 1    | 0   |\n",
    "| end     | 0     | 0   | 1    | 0       | 0     | 0    | 0     | 0   | 1    | 0   |\n",
    "| well    | 0     | 0   | 1    | 0       | 0     | 0    | 1     | 1   | 0    | 1   |\n",
    "| END     | 0     | 0   | 0    | 0       | 0     | 0    | 0     | 0   | 1    | 0   |\n",
    "\n",
    "**Note**: In NLP, we often add START and END tokens to handle edge cases at the beginning and end of sentences, paragraphs, etc. so we imagine START and END tokens encapsulating each document here, i.e. \"START All that glitters isn't gold END\".\n",
    "\n",
    "Now, we have word vectors rooted in word-word comparisons, but these vectors will be large (linear with number of distinct words in a corpus). Thus, our next step is to run dimension reduction, i.e. PCA (Principal Components Analysis) or SVD (Singular Value Decomposition) to select the top $k$ principal components, e.g. $U_k$ in the image below, as our k-dimensional word vectors.\n",
    "\n",
    "For reference, here's a visualization of dimensionality reduction with SVD:\n",
    "![alt text](./imgs/svd.png \"SVD\")\n",
    "\n",
    "This reduced-dimensionality-co-occurrence representation helps us preserve semantic relationships between words, e.g. doctor and hospital will be closer than doctor and dog. In practice, this method is not tractable for large corpora because of the memory needed to perform PCA or SVD.\n",
    "\n",
    "**Note**: If you're curious and want to learn more about PCA or SVD feel free to checkout (5 - 7). These course notes from CS168 provide a great high-level treatment of these general purpose algorithms. Though, for the purpose of this class, you only need to know how to extract the k-dimensional embeddings by utilizing pre-programmed implementations of either algorithm from the numpy, scipy, or sklearn python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Plotting - Co-Occurrence, Word Embeddings [code] (10 Points)\n",
    "1. Here, we will be using the reuters corpus. If you haven't run the import cell at the top of this page, please run it now. The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide the read_corpus function that pulls out articles from the \"coffee\" category. \n",
    "3. Construct a method that constructs a co-occurrence matrix with a fixed window of 4, considering words 4 before and after the word upon which the window is centered.\n",
    "4. Construct a method that performs dimension reduction on the matrix to produce k-dimensional embeddings, i.e. use PCA or SVD to take the top k-principal components and produce a new matrix of k-dimensional embeddings.\n",
    "5. Compute the co-occurrence matrix with fixed window of 4, over the corpus. Then compute k=2, 2-dimensional embeddings.\n",
    "6. Plot the 2D embeddings for `[\"japan\", \"america\", \"mexico\", \"canada\", \"coffee\", \"tea\"]`\n",
    "\n",
    "**Note**: We have provided a `read_corpus` function below. It already adds START and END tokens to each of the documents. You do **not** have to lemmatize the strings in the documents or perform any other form of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(category=\"coffee\"):\n",
    "    \"\"\" Read files from the specified reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            list of distinct words, number of distinct words\n",
    "    \"\"\"\n",
    "    # ------------------\n",
    "    # Your implementation here.\n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus with a window size of 4.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            numpy matrix (number of words, number of words) of co-occurrence counts,\n",
    "            dictionary mapping row/col index -> word\n",
    "    \"\"\"\n",
    "    # ------------------\n",
    "    # Your implementation here.\n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce dimensionality of (m,n) matrix to (m,k) using dimension reduction techniques like PCA or SVD.\n",
    "    \n",
    "        Possible Dimension Reduction Functions:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix): co-occurrence numpy matrix\n",
    "            k (int): number of dimensions we want for dimension reduction\n",
    "        Return:\n",
    "            numpy matrix (m,k) -- our k-dim word embeddings\n",
    "    \"\"\"    \n",
    "\n",
    "    # ------------------\n",
    "    # Your implementation here.\n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M, word2Ind,\n",
    "                    words=[\"japan\", \"america\", \"mexico\", \"canada\", \"coffee\", \"tea\"]):\n",
    "    \"\"\" Plot Embeddings of specified words (scatterplot).\n",
    "        Params:\n",
    "            M (numpy matrix -- (m, k)): embeddings\n",
    "            word2Ind (map string -> int): word to index mapping\n",
    "            words (list of strings): words whose embeddings we want to visualize\n",
    "        Return:\n",
    "            None\n",
    "            \n",
    "        Hint: Try using Matplotlib to make a scatterplot.\n",
    "    \"\"\"\n",
    "    # ------------------\n",
    "    # Your implementation here.\n",
    "    \n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing Your Plots -- Run Cell\n",
    "corpus = read_corpus()\n",
    "M, word2Ind = compute_co_occurrence_matrix(corpus)\n",
    "M_hat = reduce_to_k_dim(M, k=2)\n",
    "plot_embeddings(M_hat, word2Ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Plotting - Co-Occurrence, Word Embeddings [written] (2 points)\n",
    "What do you notice in the plot? What clusters together in 2-dimensional embedding space? What doesn't cluster together?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prediction-Based Word Vectors\n",
    "As discussed in class, more recently prediction-based word vectors have come into fashion, e.g. Word2Vec. Here, we shall explore the embeddings produced by Word2Vec. Please make sure that you have downloaded the Word2Vec embeddings from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "Please revisit the class notes and lecture slides for more details on the word2vec algorithm. If you're feeling adventurous, challenge yourself and try reading the original paper (12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Plotting -  Word2Vec, Word Embeddings [code]\n",
    "Let's directly compare the word2Vec embeddings to those of the co-occurrence matrix.\n",
    "\n",
    "1. Read the word2Vec vectors into memory.\n",
    "2. Convert the (n, 300) dimension vector to (n,2) using the same dimension reduction techniques as earlier (SVD, PCA, etc.)\n",
    "3. Plot the 2D embeddings for `[\"japan\", \"america\", \"mexico\", \"canada\", \"coffee\", \"tea\"]`\n",
    "\n",
    "**Note**: Running the `load_word2vec` function may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this variable with path to your downloaded and unzipped embeddings\n",
    "# The file should be of format \".bin\"\n",
    "embeddings_fp = \"/Users/Sahil/Desktop/cs224n-1819/assignments/prototypes/hw1/soln/embeddings/GoogleNews-vectors-negative300.bin\"\n",
    "def load_word2vec(words, embeddings_fp=embeddings_fp):\n",
    "    \"\"\" Load Word2Vec Vectors\n",
    "        Param:\n",
    "            words (list of strings) - words that we must consider in vocabulary\n",
    "            embeddings_fp (string) - path to .bin file of pretrained word vectors\n",
    "        Return:\n",
    "            numpy matrix (number of words, 300) of embeddings,\n",
    "            map of row/col index -> word,\n",
    "            KeyedVectors format of embeddings (doc https://radimrehurek.com/gensim/models/deprecated/keyedvectors.html)\n",
    "    \"\"\"\n",
    "    embed_size = 300\n",
    "    wv_from_bin = KeyedVectors.load_word2vec_format(datapath(embeddings_fp), binary=True)\n",
    "    vocab = list(wv_from_bin.vocab.keys())\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in np.random.choice(np.array(vocab).flatten(), 1000):\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue       \n",
    "    return np.stack(M), word2Ind, wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run cell to load word vectors\n",
    "## Note: This may take several minutes\n",
    "words=[\"japan\", \"america\", \"mexico\", \"canada\", \"coffee\", \"tea\"]\n",
    "M, word2Ind, wv_from_bin = load_word2vec(words)\n",
    "M_hat = reduce_to_k_dim(M, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run cell to plot word vectors\n",
    "plot_embeddings(M_hat, word2Ind, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Plotting -  Word2Vec, Word Embeddings [written] (2 points)\n",
    "Are the clusters in this plot any different than before with the co-occurrence plot?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "Now, that we have word vectors we need a way to quantify the similarity between individual words, according to these metric-spaces. One such metric is cosine-similarity. We will be using this in the following problem in order to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:\n",
    "\n",
    "![alt text](./imgs/inner_product.png \"Angle\")\n",
    "\n",
    "Instead of computing the actual angle, we can similarly leave the similarity in terms of $similarity = cos(\\Theta)$. Formally the Cosine Similarity between two vectors $p$ and $q$ is defined as (10, 11):\n",
    "\n",
    "$$Cosine\\_Dist = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } Cosine\\_Dist \\in [-1, 1] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Synonyms & Antonyms - Word2Vec, Word Embeddings [code + written] (8 points)\n",
    "1. Find a polysemous word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words contains related words from both meanings. For example, \"leaves\" has both \"vanishes\" and \"stalks\" in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous words before you find one. Please state the polysemous word you discover and the various definitions that are surfaced in your exploration of similar words. Why do you think many of the polysemous words you tried didn't work? **(4 points)**\n",
    "\n",
    "**Note**: The `wv_from_bin.most_similar(word)` functions may be helpful here. It ranks all other words with respect to their cosine similarity to a given word. Please see https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors for more documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code for polysemous word exploration here\n",
    "## ----------------------------------------------\n",
    "\n",
    "## ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Answer Here (Polysemous Word Exploration)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but dist(w1,w3) < dist(w1,w2). For example, \"qualified\" is closer to \"unqualified\" than to \"skilled\". Can you give a possible explanation for why this happens? **(4 points)**\n",
    "\n",
    "**Note**: The `wv_from_bin.distance(w1, w2)` function may be helpful here in order to compute the cosine distance between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code for snonym/antonym word exploration here\n",
    "## ----------------------------------------------\n",
    "\n",
    "## ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Answer Here (Synonym/Antonym Word Exploration)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Analogies - Word2Vec, Word Embeddings [code] (4 points)\n",
    "Interestingly Word2Vec vectors have been shown to sometime be able to exhibit analogies. Please use the `most_similar(positive=[], negative=[])` function which finds words who are most similar to the words in the `positive` list and and most dissimilar from the words in the `negative` list.\n",
    "\n",
    "1. For the analogy \"man : king :: woman : x\", what is x? In the cell below, we show you how to adapt this analogy into the `most_similar(positive=[], negative=[])` syntax. **(1 point)**\n",
    "\n",
    "**Note:** Documentation on the `most_similar` function can be found at https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write X (According to Word Vectors) Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find an example of analogy that holds according to these vectors. State analogy solution, according to the word vectors. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert code to demonstrate working analogy here\n",
    "## ----------------------------------------------\n",
    "\n",
    "## ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Analogy (According to Word Vectors) Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find an example of analogy that does not hold according to these vectors. State the intended analogy and analogy solution, according to the word vectors. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert code to demonstrate broken analogy here\n",
    "## ----------------------------------------------\n",
    "\n",
    "## ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Intended Analogy & Compare with Closest Words (According to Word Vectors) Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Bias - Word2Vec, Word Embeddings [code + witten] (4 points)\n",
    "It's important to be cognizant of the biases implicit to our word embeddings.\n",
    "\n",
    "1. Run the cell below, to examine which terms are most similar to \"woman\" and \"boss\" and most dissimilar from \"man\". What do you find? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'boss'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain Your Findings Here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the `most_similar` function to find another case where bias is exhibited by the vectors. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert code to demonstrate bias\n",
    "## ----------------------------------------------\n",
    "\n",
    "## ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Your Example of Bias Here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What might be leading to the development of these biases in the word vectors? **(2 points)**\n",
    "\n",
    "**Write Your Answer Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Word_embedding\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/John_Rupert_Firth\n",
    "\n",
    "[3] http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf\n",
    "\n",
    "[4] https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285\n",
    "\n",
    "[5] https://web.stanford.edu/class/cs168/l/l7.pdf\n",
    "\n",
    "[6] http://theory.stanford.edu/~tim/s15/l/l8.pdf\n",
    "\n",
    "[7] https://web.stanford.edu/class/cs168/l/l9.pdf\n",
    "\n",
    "[8] https://en.wikipedia.org/wiki/Taxicab_geometry\n",
    "\n",
    "[9] https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "\n",
    "[10] https://en.wikipedia.org/wiki/Inner_product_space\n",
    "\n",
    "[11] https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "[12] https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
